{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–µ–∫—Ç —Å–¥–µ–ª–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞:\n",
    "- https://github.com/evgeny-ydsc/first-project/blob/main/—à–∞–±–ª–æ–Ω%20–ø—Ä–æ–µ–∫—Ç–∞/—à–∞–±–ª–æ–Ω%20–ø—Ä–æ–µ–∫—Ç–∞.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\n",
    "\n",
    "## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "\n",
    "\n",
    "- **–¶–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_TASK = 1\n",
    "REGRESSION_TASK = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phik -q\n",
    "import phik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap -q\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION_TASK:\n",
    "    !pip install lightgbm -q\n",
    "    import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION_TASK:\n",
    "    !pip install category_encoders -q\n",
    "    from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path (csv_name, folder=\"\"):\n",
    "    if not os.path.exists(folder + csv_name):\n",
    "        return '/datasets/' + csv_name\n",
    "    return folder + csv_name\n",
    "\n",
    "def def_load_csv(csv, folder=\"\", print_info=True, **kwargs):\n",
    "    display(f\"‚¨á-------- –¢–∞–±–ª–∏—Ü–∞ {csv}: ----------‚¨á\")\n",
    "    df = pd.read_csv(correct_path(csv, folder), **kwargs)\n",
    "    if print_info:\n",
    "        display(df.info())\n",
    "        display(df.head())\n",
    "    return df\n",
    "\n",
    "def columns_to_snake_case (df):\n",
    "    names = {}\n",
    "    for c in df.columns:\n",
    "        names [c] = re.sub(r'(?<!^)(?=[A-Z])', '_', c).lower() \\\n",
    "            .translate(str.maketrans('','',' (){}[]<>/\\\\'))\n",
    "    df.rename(columns=names, inplace=True)\n",
    "    return df\n",
    "\n",
    "# –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –≤ –≤–∏–¥–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã TOP-–∑–Ω–∞—á–µ–Ω–∏–π\n",
    "# –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –≤ –≤–∏–¥–µ —è—â–∏–∫–æ–≤ —Å —É—Å–∞–º–∏ –∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –≤—ã–≤–æ–¥–∏—Ç descirbe\n",
    "# –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É –∫–∞–∫ –∑–∞–ø—Ä–æ—Å –≤ query()\n",
    "# –µ—Å–ª–∏ sort_filter_index=-1, —Ç–æ –ø–æ—Ä—è–¥–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏—Ö –¥–ª–∏–Ω–µ (—Å—É–º–º–∞—Ä–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º)\n",
    "# ignore_columns –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Ä–∞—Å—Å—á—ë—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "def do_eda(df, columns=[], ignore_columns=[\"id\"], filters=[], sort_filter_index=-1, top_categories=10):\n",
    "    bins=50\n",
    "    n_filters = len(filters)\n",
    "    \n",
    "    if len(columns) == 0:\n",
    "        columns = df.columns\n",
    "\n",
    "    # –≤—ã–≤–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º —Ñ–∏—á–∞–º \n",
    "    for column in columns:\n",
    "        if ( df[column].dtype == 'O' or df[column].dtype.name == 'category') \\\n",
    "                and column not in ignore_columns:\n",
    "            display (f\"‚¨á-------- —Å—Ç–æ–ª–±–µ—Ü {column} --------‚¨á\")\n",
    "            title0 = f\"—Å—Ç–æ–ª–±–µ—Ü {column}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –¥–æ–ª—è –∑–Ω–∞—á–µ–Ω–∏–π\"\n",
    "            if n_filters == 0:\n",
    "                df_show = pd.DataFrame(df.groupby(column)[column].count(),\n",
    "                    columns = [column])\n",
    "                df_show.index.name = ''\n",
    "                df_show = df_show.sort_values( column, ascending=False)\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i, _ in enumerate(filters):\n",
    "                    df_f = df.query(filters[i]).groupby(column)[column].count()\n",
    "                    df_f = pd.DataFrame({f'—Ñ–∏–ª—å—Ç—Ä: {filters[i]}':df_f})\n",
    "                    df_f.index.name = ''\n",
    "                    df_show = df_show.join(df_f, how='outer')\n",
    "                df_show = df_show.fillna(0)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show['all_cat_agg_tmp'] = df_show.sum(axis=1)\n",
    "                df_show = df_show.sort_values( \\\n",
    "                    df_show.columns[sort_filter_index], ascending=False)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show = df_show.drop (columns=['all_cat_agg_tmp'], axis=1)\n",
    "            # sum not-top categories in a single row\n",
    "            if top_categories + 1 < df_show.shape[0]:\n",
    "                df_show_other = df_show.iloc [top_categories:]\n",
    "                df_show = df_show.iloc [:top_categories]\n",
    "                df_show.loc[f'All other {df_show_other.shape[0]} categories'] = df_show_other.sum()\n",
    "\n",
    "            ax = df_show.plot.barh(title=title0, stacked=(n_filters != 0));\n",
    "            ax.invert_yaxis()\n",
    "            df_show ['sum_cols'] = df_show.sum(axis=1)\n",
    "            sum_all = df_show.iloc[:,-1].sum()\n",
    "            for i in range(df_show.shape[0]):\n",
    "                sum_row = df_show.iloc[i, -1]\n",
    "                s = f'{sum_row} ({int(sum_row*100.0/sum_all)}'\n",
    "                for j in range(n_filters):\n",
    "                    s += '=' if j == 0 else '+'\n",
    "                    s += f'{int(df_show.iloc[i,j]*100.0/sum_all)}'\n",
    "                ax.text(int(sum_row*1.05), i, s+')%')\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ax.set_xlim(xmin, 1.5*xmax)\n",
    "            ax.legend(bbox_to_anchor=(1.8, 0.7))\n",
    "            plt.show();\n",
    "\n",
    "    # –≤—ã–≤–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ —á–∏—Å–ª–æ–≤—ã–º —Ñ–∏—á–∞–º \n",
    "    for column in columns:\n",
    "        if df[column].dtype in ['int64', 'float64'] and column not in ignore_columns:\n",
    "            display (f\"‚¨á-------- –°–ª–µ–¥—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫: {column} --------‚¨á\")\n",
    "            title = f\"—Å—Ç–æ–ª–±–µ—Ü {column}\"\n",
    "            _, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 7))\n",
    "            if n_filters == 0:\n",
    "                df_show = df[[column]]\n",
    "                display(df_show.describe())\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i in range (n_filters):\n",
    "                    df_f = df.query(filters[i]).copy()\n",
    "                    f_title = f\"—Ñ–∏–ª—å—Ç—Ä: {filters[i]}\"\n",
    "                    df_f.loc[:, 'filter'] = f_title\n",
    "                    df_f = df_f[['filter', column]]\n",
    "                    df_show = pd.concat([df_show, df_f], axis=0, ignore_index=True)\n",
    "                    display(f_title)\n",
    "                    display(df_f[column].describe())\n",
    "\n",
    "            ax = sns.boxplot(data=df_show, orient=\"h\", ax=axs[0], y = None if n_filters == 0 else 'filter', x = column)\n",
    "            sns.histplot(data=df_show, x=column, hue= None if n_filters == 0 else 'filter', multiple=\"dodge\",\n",
    "                        bins=bins, kde=True, ax=axs[1], alpha=0.8, legend=False)\n",
    "            plt.show();\n",
    "\n",
    "def display_drop_full_duplicates(df):\n",
    "    display(f'{df.duplicated().sum()} full duplicates are dropped')\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def print_unique_text_features(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            x = df[c].unique()\n",
    "            x = np.sort(x[~pd.isnull(x)])\n",
    "            display(f\"‚¨á-------- Feature \\\"{c}\\\"--------‚¨á\")\n",
    "            display(x)\n",
    "\n",
    "# example:\n",
    "# df = split_date (df, 'created_at', needed_parts='ymdwh', format='%Y-%m-%d %H:%M:%S'),\n",
    "# use it after solving NaNs to prevent float types of the new fields\n",
    "def split_date (df, date_column, needed_parts, format=\"\", drop_date_column=False):\n",
    "    if format != \"\":\n",
    "        df[date_column] = pd.to_datetime(df[date_column], format=format)\n",
    "    if 'y' in needed_parts:\n",
    "        df [date_column + \"_year\"] = df[date_column].dt.year\n",
    "    if 'm' in needed_parts:\n",
    "        df [date_column + \"_month\"] = df[date_column].dt.month\n",
    "    if 'd' in needed_parts:\n",
    "        df [date_column + \"_day\"] = df[date_column].dt.day\n",
    "    if 'w' in needed_parts:\n",
    "        df [date_column + \"_weekday\"] = df[date_column].dt.weekday\n",
    "    if 'h' in needed_parts:\n",
    "        df [date_column + \"_hour\"] = df[date_column].dt.hour\n",
    "    if drop_date_column:\n",
    "        df = df.drop([date_column], axis=1)\n",
    "    return df\n",
    "\n",
    "def show_nans (df):\n",
    "    n = df.isna().sum()\n",
    "    l = df.shape[0]\n",
    "    display(\"empty parts:\")\n",
    "    display(n [n>0].map(lambda x: f'{round(x/l*100, 2)}%'))\n",
    "\n",
    "def fill_by_median (df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "def fill_by_value (df, columns, val):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(val)\n",
    "    return df\n",
    "\n",
    "def get_multicollinear_features (phik_matrix):\n",
    "    mc_features=[]\n",
    "    while True:\n",
    "        if phik_matrix.shape[0] == 0:\n",
    "            break\n",
    "        mc_features_ranked = phik_matrix[(phik_matrix >= 0.9) & (phik_matrix < 1)].sum()\n",
    "        mc_features_ranked = mc_features_ranked[mc_features_ranked>0] \\\n",
    "            .sort_values(ascending=False)\n",
    "        if mc_features_ranked.shape[0] == 0:\n",
    "            break\n",
    "        top_mc_feature = mc_features_ranked.index[0]\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=0)\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=1)\n",
    "        mc_features.append(top_mc_feature)\n",
    "    return mc_features, phik_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–ª–∞–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path (csv_name, folder=\"\"):\n",
    "    if not os.path.exists(folder + csv_name):\n",
    "        return '/datasets/' + csv_name\n",
    "    return folder + csv_name\n",
    "\n",
    "def def_load_csv(csv, folder=\"\", print_info=True):\n",
    "    display(f\"‚¨á-------- –¢–∞–±–ª–∏—Ü–∞ {csv}: ----------‚¨á\")\n",
    "    df = pd.read_csv(correct_path(csv, folder))\n",
    "    if print_info:\n",
    "        display(df.info())\n",
    "        display(df.head())\n",
    "    return df\n",
    "\n",
    "def columns_to_snake_case (df):\n",
    "    names = {}\n",
    "    for c in df.columns:\n",
    "        names [c] = re.sub(r'(?<!^)(?=[A-Z])', '_', c).lower() \\\n",
    "            .translate(str.maketrans('','',' (){}[]<>/\\\\'))\n",
    "    df.rename(columns=names, inplace=True)\n",
    "    return df\n",
    "\n",
    "# –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –≤ –≤–∏–¥–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã TOP-–∑–Ω–∞—á–µ–Ω–∏–π\n",
    "# –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –≤ –≤–∏–¥–µ —è—â–∏–∫–æ–≤ —Å —É—Å–∞–º–∏ –∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, –≤—ã–≤–æ–¥–∏—Ç descirbe\n",
    "# –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤, —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º—É –∫–∞–∫ –∑–∞–ø—Ä–æ—Å –≤ query()\n",
    "# –µ—Å–ª–∏ sort_filter_index=-1, —Ç–æ –ø–æ—Ä—è–¥–æ–∫ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏—Ö –¥–ª–∏–Ω–µ (—Å—É–º–º–∞—Ä–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä–∞–º)\n",
    "# ignore_columns –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å —Ä–∞—Å—Å—á—ë—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è —Å—Ç–æ–ª–±—Ü–æ–≤-–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "def do_eda(df, columns=[], ignore_columns=[\"id\"], filters=[], sort_filter_index=-1, top_categories=10):\n",
    "    bins=25\n",
    "    n_filters = len(filters)\n",
    "    \n",
    "    if len(columns) == 0:\n",
    "        columns = df.columns\n",
    "\n",
    "    # –≤—ã–≤–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º —Ñ–∏—á–∞–º \n",
    "    for column in columns:\n",
    "        if ( df[column].dtype == 'O' or df[column].dtype.name == 'category') \\\n",
    "                and column not in ignore_columns:\n",
    "            display (f\"‚¨á-------- —Å—Ç–æ–ª–±–µ—Ü {column} --------‚¨á\")\n",
    "            title0 = f\"—Å—Ç–æ–ª–±–µ—Ü {column}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ –¥–æ–ª—è –∑–Ω–∞—á–µ–Ω–∏–π\"\n",
    "            if n_filters == 0:\n",
    "                df_show = pd.DataFrame(df.groupby(column)[column].count(),\n",
    "                    columns = [column])\n",
    "                df_show.index.name = ''\n",
    "                df_show = df_show.sort_values( column, ascending=False)\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i, _ in enumerate(filters):\n",
    "                    df_f = df.query(filters[i]).groupby(column)[column].count()\n",
    "                    df_f = pd.DataFrame({f'—Ñ–∏–ª—å—Ç—Ä: {filters[i]}':df_f})\n",
    "                    df_f.index.name = ''\n",
    "                    df_show = df_show.join(df_f, how='outer')\n",
    "                df_show = df_show.fillna(0)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show['all_cat_agg_tmp'] = df_show.sum(axis=1)\n",
    "                df_show = df_show.sort_values( \\\n",
    "                    df_show.columns[sort_filter_index], ascending=False)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show = df_show.drop (columns=['all_cat_agg_tmp'], axis=1)\n",
    "            # sum not-top categories in a single row\n",
    "            if top_categories + 1 < df_show.shape[0]:\n",
    "                df_show_other = df_show.iloc [top_categories:]\n",
    "                df_show = df_show.iloc [:top_categories]\n",
    "                df_show.loc[f'All other {df_show_other.shape[0]} categories'] = df_show_other.sum()\n",
    "\n",
    "            ax = df_show.plot.barh(title=title0, stacked=(n_filters != 0));\n",
    "            ax.invert_yaxis()\n",
    "            df_show ['sum_cols'] = df_show.sum(axis=1)\n",
    "            sum_all = df_show.iloc[:,-1].sum()\n",
    "            for i in range(df_show.shape[0]):\n",
    "                sum_row = df_show.iloc[i, -1]\n",
    "                s = f'{sum_row} ({int(sum_row*100.0/sum_all)}'\n",
    "                for j in range(n_filters):\n",
    "                    s += '=' if j == 0 else '+'\n",
    "                    s += f'{int(df_show.iloc[i,j]*100.0/sum_all)}'\n",
    "                ax.text(int(sum_row*1.05), i, s+')%')\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ax.set_xlim(xmin, 1.5*xmax)\n",
    "            ax.legend(bbox_to_anchor=(1.8, 0.7))\n",
    "            plt.show();\n",
    "\n",
    "    # –≤—ã–≤–æ–¥ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ —á–∏—Å–ª–æ–≤—ã–º —Ñ–∏—á–∞–º \n",
    "    for column in columns:\n",
    "        if df[column].dtype in ['int64', 'float64'] and column not in ignore_columns:\n",
    "            display (f\"‚¨á-------- –°–ª–µ–¥—É—é—â–∏–π –ø—Ä–∏–∑–Ω–∞–∫: {column} --------‚¨á\")\n",
    "            title = f\"—Å—Ç–æ–ª–±–µ—Ü {column}\"\n",
    "            _, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 7))\n",
    "            if n_filters == 0:\n",
    "                df_show = df[[column]]\n",
    "                display(df_show.describe())\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i in range (n_filters):\n",
    "                    df_f = df.query(filters[i]).copy()\n",
    "                    f_title = f\"—Ñ–∏–ª—å—Ç—Ä: {filters[i]}\"\n",
    "                    df_f.loc[:, 'filter'] = f_title\n",
    "                    df_f = df_f[['filter', column]]\n",
    "                    df_show = pd.concat([df_show, df_f], axis=0, ignore_index=True)\n",
    "                    display(f_title)\n",
    "                    display(df_f[column].describe())\n",
    "\n",
    "            ax = sns.boxplot(data=df_show, orient=\"h\", ax=axs[0], y = None if n_filters == 0 else 'filter', x = column)\n",
    "            sns.histplot(data=df_show, x=column, hue= None if n_filters == 0 else 'filter', multiple=\"dodge\",\n",
    "                        bins=bins, kde=True, ax=axs[1], alpha=0.8, legend=False)\n",
    "            plt.show();\n",
    "\n",
    "def display_drop_full_duplicates(df):\n",
    "    display(f'{df.duplicated().sum()} full duplicates are dropped')\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def print_unique_text_features(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            x = df[c].unique()\n",
    "            x = np.sort(x[~pd.isnull(x)])\n",
    "            display(f\"‚¨á-------- Feature \\\"{c}\\\"--------‚¨á\")\n",
    "            display(x)\n",
    "\n",
    "# example:\n",
    "# df = split_date (df, 'created_at', needed_parts='ymdwh', format='%Y-%m-%d %H:%M:%S'),\n",
    "# use it after solving NaNs to prevent float types of the new fields\n",
    "def split_date (df, date_column, needed_parts, format=\"\", drop_date_column=False):\n",
    "    if format != \"\":\n",
    "        df[date_column] = pd.to_datetime(df[date_column], format=format)\n",
    "    if 'y' in needed_parts:\n",
    "        df [date_column + \"_year\"] = df[date_column].dt.year\n",
    "    if 'm' in needed_parts:\n",
    "        df [date_column + \"_month\"] = df[date_column].dt.month\n",
    "    if 'd' in needed_parts:\n",
    "        df [date_column + \"_day\"] = df[date_column].dt.day\n",
    "    if 'w' in needed_parts:\n",
    "        df [date_column + \"_weekday\"] = df[date_column].dt.weekday\n",
    "    if 'h' in needed_parts:\n",
    "        df [date_column + \"_hour\"] = df[date_column].dt.hour\n",
    "    if drop_date_column:\n",
    "        df = df.drop([date_column], axis=1)\n",
    "    return df\n",
    "\n",
    "def show_nans (df):\n",
    "    n = df.isna().sum()\n",
    "    l = df.shape[0]\n",
    "    display(\"empty parts:\")\n",
    "    display(n [n>0].map(lambda x: f'{round(x/l*100, 2)}%'))\n",
    "\n",
    "def fill_by_median (df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "def fill_by_value (df, columns, val):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(val)\n",
    "    return df\n",
    "\n",
    "def get_multicollinear_features (phik_matrix):\n",
    "    mc_features=[]\n",
    "    while True:\n",
    "        if phik_matrix.shape[0] == 0:\n",
    "            break\n",
    "        mc_features_ranked = phik_matrix[(phik_matrix >= 0.9) & (phik_matrix < 1)].sum()\n",
    "        mc_features_ranked = mc_features_ranked[mc_features_ranked>0] \\\n",
    "            .sort_values(ascending=False)\n",
    "        if mc_features_ranked.shape[0] == 0:\n",
    "            break\n",
    "        top_mc_feature = mc_features_ranked.index[0]\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=0)\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=1)\n",
    "        mc_features.append(top_mc_feature)\n",
    "    return mc_features, phik_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–±–ª–∏—Ü, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ–ø–∞—Ä–∞—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = def_load_csv ('....csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–ø—Ä–∞–≤–∏–º –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = columns_to_snake_case(df)\\ndisplay (df.info())\\ndisplay (df.head())\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = columns_to_snake_case(df)\n",
    "display (df.info())\n",
    "display (df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Å–ø—Ä–∞–≤–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['...'] = pd.to_datetime( df['...'], format='%Y-%m-%d %H:%M:%S' )\n",
    "#df['...'] = df['...'].astype('object')\n",
    "#df['..._year'] = df['...'].dt.year\n",
    "#df['..._month'] = df['...'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndisplay(df.info())\\ndisplay(df.head())\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "display(df.info())\n",
    "display(df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –£–¥–∞–ª–∏–º –ø–æ–ª—è, –ø—Ä–∏–≤–æ–¥—è—â–∏–µ –∫ —É—Ç–µ—á–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∑–∞–≤–µ–¥–æ–º–æ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `...` - ... –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['...', '...'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–∑—É—á–∏–º –ø—Ä–æ–ø—É—Å–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = df.isna().sum()\\nl = len(df)\\nround(n [n>0] / l, 2)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "show_nans (df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf['vehicle_type'] = df['vehicle_type']     .fillna(df.groupby(['brand', 'model'])['vehicle_type']             .transform(lambda x: x.mode()[0] if not x.mode().empty else None))\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df['vehicle_type'] = df['vehicle_type'] \\\n",
    "    .fillna(df.groupby(['brand', 'model'])['vehicle_type'] \\\n",
    "            .transform(lambda x: x.mode()[0] if not x.mode().empty else None))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑üö©[todo] –µ—Å–ª–∏ –∂–µ –∫–∞–∫–∏–µ-—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∏ –≤—Å—ë-—Ç–∞–∫–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è - –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –Ω–∏—Ö —Å –ø–æ–º–æ—â—å—é imputer-–æ–≤ –≤ pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–∑—É—á–∏–º –∏ —É–¥–∞–ª–∏–º —è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndisplay(df.duplicated().sum())\\ndf = df.drop_duplicates()\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "display(df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò–∑—É—á–∏–º –Ω–µ—è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_unique_text_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–º–µ–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏—è xxx —Ñ–∏—á–∏ f –Ω–∞ yyy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['f']=='xxx', 'f'] = 'yyy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ä–∞–≤–Ω–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∏—á–∏ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –∏ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∞—Ö –µ—Å–ª–∏ –æ–Ω–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –∞ –Ω–µ –∏–∑ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã(—Å–æ—Å—Ç–∞–≤, —Ç–∏–ø—ã, –∑–Ω–∞—á–µ–Ω–∏—è) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã –∏ –∑–∞–¥–∞—á–∏ –ø–æ –∏—Ç–æ–≥–∞–º EDA\n",
    "1. ...\n",
    "    1. üë∑üö©[todo] ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, –∑–∞–º–µ–Ω–∏–≤ –∞–Ω–æ–º–∞–ª–∏–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf.loc[(df['xxx'] <10) | (df['xxx'] > 1600), 'xxx'] =     df.groupby(['brand', 'model'])['power']         .transform(lambda x: x.mode()[0] if not x.mode().empty else None)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df.loc[(df['xxx'] <10) | (df['xxx'] > 1600), 'xxx'] = \\\n",
    "    df.groupby(['brand', 'model'])['power'] \\\n",
    "        .transform(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, —É–¥–∞–ª–∏–≤ –∞–Ω–æ–º–∞–ª–∏–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.query('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolumns_to_delete = ['...', '...']\\nX_train = X_train.drop(columns_to_delete, axis=1)\\nX_test = X_test.drop(columns_to_delete, axis=1)\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "columns_to_delete = ['...', '...']\n",
    "df = df.drop(columns_to_delete, axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n_, ax = plt.subplots(figsize=(25,25))\\nph_mx = X_train.phik_matrix(interval_cols=['kilometer', 'power', 'registration_year'],\\n                bins={'kilometer':10, 'power':10, 'registration_year':10})\\nsns.heatmap(ph_mx, annot=True, fmt='.2f', ax=ax);\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "interval_cols=['xxx', 'xxx', 'xxx', 'xxx']\n",
    "   \n",
    "bins = {}\n",
    "for c in interval_cols:\n",
    "    bins[c] = 10;\n",
    "\n",
    "_, ax = plt.subplots(figsize=(25,25))\n",
    "ph_mx = X_train.phik_matrix(interval_cols=interval_cols, bins=bins)\n",
    "sns.heatmap(ph_mx, annot=True, fmt='.2f', ax=ax);\n",
    "\n",
    "columns_to_delete, ph_mx_no_mc = get_multicollinear_features (ph_mx)\n",
    "display (\"–ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:\")\n",
    "display (columns_to_delete)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*–ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –æ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –≥–æ–≤–æ—Ä—è—Ç –ø—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –æ—Ç 0.9 –¥–æ 0.95 –ø–æ –º–æ–¥—É–ª—é. –í —Ç–∞–∫–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –ª–∏—à–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–∂–Ω–æ —É–¥–∞–ª—è—Ç—å –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolumns_to_delete = ['...', '...']\\nX_train = X_train.drop(columns_to_delete, axis=1)\\nX_test = X_test.drop(columns_to_delete, axis=1)\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "columns_to_delete = ['...', '...']\n",
    "X_train = X_train.drop(columns_to_delete, axis=1)\n",
    "X_test = X_test.drop(columns_to_delete, axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–∞–∑–æ–±—ä—ë–º –≤—ã–±–æ—Ä–∫—É –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é (—Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ —Ü–µ–ª–µ–≤–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –≤ —Å–ª—É—á–∞–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX = df.drop(['price'], axis=1)\\ny = df['price']\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, \\n    y, \\n    random_state=RANDOM_STATE\\n)\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∏–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞ (–≤ —Å–ª—É—á–∞–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_columns = ['...', '...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor c in X_train.columns:\\n    if X_train[c].dtype == \\'O\\':\\n        X_train[c] = X_train[c].astype(\"category\")\\ndisplay(X_train.info())\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype == 'O':\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "display(X_train.info())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cat_columns = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "ord_columns = ['weather_1', 'road_surface', 'road_condition_1', 'lighting ', 'control_device']\n",
    "cat_columns = list(set(cat_columns) - set(ord_columns))\n",
    "num_features = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncat_pipe = Pipeline(\\n    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\\n     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\\n    ]\\n    )\\n\\ndata_preprocessor = ColumnTransformer(\\n    [\\n        ('ohe', cat_pipe, cat_columns),\\n        ('num', StandardScaler(), num_columns),\\n    ], \\n    remainder='passthrough'\\n) \\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ohe_pipe = Pipeline(\n",
    "    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "ord_pipe = Pipeline(\n",
    "    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "     ('ord',  OrdinalEncoder(\n",
    "                categories=[\n",
    "                    ['clear', 'cloudy', 'other', '-', 'wind', 'raining', 'snowing', 'fog'], \n",
    "                    ['dry', 'wet', '-', 'slippery', 'snowy'], \n",
    "                    ['normal', 'other', '-', 'reduced width', 'flooded', 'holes', 'loose material', 'obstruction', 'construction'], \n",
    "                    ['daylight', 'dark with street lights', '-', 'dark with street lights not functioning', 'dark with no street lights', 'dusk or dawn'], \n",
    "                    ['functioning', 'obscured', 'not functioning', 'none']\n",
    "                ], \n",
    "                handle_unknown='use_encoded_value', unknown_value=np.nan\n",
    "            )\n",
    "        ),\n",
    "     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('ohe', ohe_pipe, cat_columns),\n",
    "        ('ord', ord_pipe, ord_columns),\n",
    "        ('num', StandardScaler(), num_columns),\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pipe_final = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "]\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\npipe_final = Pipeline([\\n    ('preprocessor', data_preprocessor),\\n    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))\\n]\\n)\\n\\nparam_grid = [\\n    {\\n        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')],\\n        'models__max_depth': [4,7],\\n        'models__max_features': [10, 50],\\n        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],\\n        'models__max_depth': [10, 20],\\n        'models__min_samples_split': [5, 10],\\n        'models__min_samples_leaf': [2, 4],\\n        'models__bootstrap': [True, False],\\n        'preprocessor__num': [MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [LogisticRegression(\\n            random_state=RANDOM_STATE, \\n            solver='liblinear', \\n            penalty='l1',\\n            class_weight='balanced'\\n        )],\\n        'models__C': range(1, 3),\\n        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [SVC(\\n            random_state=RANDOM_STATE, \\n            kernel='linear', probability=True, class_weight='balanced'\\n        )],\\n        'models__kernel': ['poly', 'rbf'],\\n        'models__degree': [2, 5, 10],\\n        'models__C': range(2, 3),\\n    }\\n]\\n\\nsearch = RandomizedSearchCV(\\n    pipe_final, \\n    param_grid,\\n    cv=3,\\n    scoring='f1',\\n    n_iter=30,\\n    verbose = 10,\\n    n_jobs=-1\\n)\\n\\nsearch.fit(X_train, y_train)\\n\\ngrid_search.best_params_\\n\\nbest_model = search.best_estimator_\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "param_grid = [\n",
    "    {\n",
    "        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__max_depth': [4,7],\n",
    "        'models__max_features': [10, 50],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],\n",
    "        'models__max_depth': [10, 20],\n",
    "        'models__min_samples_split': [5, 10],\n",
    "        'models__min_samples_leaf': [2, 4],\n",
    "        'models__bootstrap': [True, False],\n",
    "        'preprocessor__num': [MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, \n",
    "            solver='liblinear', \n",
    "            penalty='l1',\n",
    "            class_weight='balanced'\n",
    "        )],\n",
    "        'models__C': range(1, 3),\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [SVC(\n",
    "            random_state=RANDOM_STATE, \n",
    "            kernel='linear', probability=True, class_weight='balanced'\n",
    "        )],\n",
    "        'models__kernel': ['poly', 'rbf'],\n",
    "        'models__degree': [2, 5, 10],\n",
    "        'models__C': range(2, 3),\n",
    "    }\n",
    "]\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe_final, \n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_iter=30,\n",
    "    verbose = 10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Å–ª—É—á–∞–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = [\n",
    "    {\n",
    "        'models': [LinearRegression()],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [DecisionTreeRegressor(random_state=RANDOM_STATE)],\n",
    "        'models__max_depth': [3, 5, 10, None],\n",
    "        'models__min_samples_split': [2, 5, 10],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    },\n",
    "    {\n",
    "        'models': [SVR()],\n",
    "        'models__kernel': ['linear', 'rbf'],\n",
    "        'models__C': [0.1, 1, 10],\n",
    "        'models__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'models': [RandomForestRegressor(random_state=RANDOM_STATE)],\n",
    "        'models__n_estimators': [50, 100],\n",
    "        'models__max_depth': [None, 10, 20],\n",
    "        'models__min_samples_split': [2, 5]\n",
    "    }\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display ('–ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –∫—Ä–æ—Å—Å–≤–∞–ª–∏–¥–∞—Ü–∏–∏: ', search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_pred = best_model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –æ—Ü–µ–Ω–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "'''\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ny_pred = best_model.predict(X_test)\\ndisplay ('RMSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ = ', root_mean_squared_error(y_test, y_pred))\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –æ—Ü–µ–Ω–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏\n",
    "'''\n",
    "y_pred = best_model.predict(X_test)\n",
    "display ('RMSE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ = ', root_mean_squared_error(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "'''\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrecisionRecallDisplay\n",
    "'''\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test);\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ê–Ω–∞–ª–∏–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ (–≤ —Å–ª—É—á–∞–µ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresiduals = y_test - y_pred\\n\\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\\naxes[0].hist(residuals, bins=33)\\naxes[0].set_title('–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤')\\naxes[0].set_xlabel('–û—Å—Ç–∞—Ç–∫–∏')\\n\\naxes[1].scatter(predictions, residuals)\\naxes[1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏')\\naxes[1].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏')\\naxes[1].set_title('–ê–Ω–∞–ª–∏–∑ –¥–∏—Å–ø–µ—Ä—Å–∏–∏')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "axes[0].hist(residuals, bins=33)\n",
    "axes[0].set_title('–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤')\n",
    "axes[0].set_xlabel('–û—Å—Ç–∞—Ç–∫–∏')\n",
    "\n",
    "axes[1].scatter(y_pred, residuals)\n",
    "axes[1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏')\n",
    "axes[1].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏')\n",
    "axes[1].set_title('–ê–Ω–∞–ª–∏–∑ –¥–∏—Å–ø–µ—Ä—Å–∏–∏')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—Ç—å—Å—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ, –∞ —Ç–∞–∫–∂–µ –∏–º–µ—Ç—å –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é –¥–∏—Å–ø–µ—Ä—Å–∏—é –¥–ª—è –≤—Å–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.\n",
    "\n",
    "–î–ª—è –≤—Å–µ—Ö —Å–ª—É—á–∞–µ–≤ –Ω–µ–Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥—è—Ç –¥–≤–∞ —Å–æ–≤–µ—Ç–∞:\n",
    "- –ù–∞–π—Ç–∏ –∏ –¥–æ–±–∞–≤–∏—Ç—å –≤ –º–æ–¥–µ–ª—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –Ω–∞ —ç—Ç–∞–ø–µ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –±—ã–ª–∞ —É—á—Ç–µ–Ω–∞ –∫–∞–∫–∞—è-—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.\n",
    "- –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –∏–∑–º–µ–Ω–∏—Ç—å –≤–∑–∞–∏–º–æ—Å–≤—è–∑—å –≤—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤—ã–º. –≠—Ç–∏–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º –≤—ã –Ω–∞—É—á–∏—Ç–µ—Å—å –≤ —Å–ª–µ–¥—É—é—â–µ–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsamples = 20\\n \\n# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RandomizedSearchCV\\nbest_model = search.best_estimator_.named_steps['models']\\n\\n# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω –±–µ–∑ –∫–æ–Ω–µ—á–Ω–æ–π –º–æ–¥–µ–ª–∏\\npreprocessor = search.best_estimator_.named_steps['preprocessor']\\nX_train_preprocessed = preprocessor.transform(X_train)\\nX_test_preprocessed = preprocessor.transform(X_test)\\n\\nall_feature_names = preprocessor.get_feature_names_out()\\n\\n# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫\\nX_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, \\n                                       columns=all_feature_names)\\nX_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, \\n                                      columns=all_feature_names)\\n\\n# –°–µ–º–ø–ª –¥–∞–Ω–Ω—ã—Ö –¥–ª—è KernelExplainer\\nX_train_preprocessed_smpl = shap.sample(X_train_preprocessed_df, \\n                                        samples, random_state=RANDOM_STATE)\\nX_test_preprocessed_smpl = shap.sample(X_test_preprocessed_df, \\n                                       samples, random_state=RANDOM_STATE)\\n\\n# –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å DataFrame —Å –∏–º–µ–Ω–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å KernelExplainer\\n###explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\\nexplainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\\nshap_values = explainer.shap_values(X_test_preprocessed_smpl)\\n\\n# –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Explanation –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ö–õ–ê–°–°–ê.\\nshap_values_explanation = shap.Explanation(\\n    values=shap_values[:,:,1], \\n    base_values=explainer.expected_value,\\n    data=X_test_preprocessed_smpl,\\n    feature_names=all_feature_names\\n)\\n\\nshap.summary_plot(shap_values_explanation, plot_size=[15,6])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "samples = 20\n",
    " \n",
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RandomizedSearchCV\n",
    "best_model = search.best_estimator_.named_steps['models']\n",
    "\n",
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω –±–µ–∑ –∫–æ–Ω–µ—á–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "preprocessor = search.best_estimator_.named_steps['preprocessor']\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "all_feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫\n",
    "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, \n",
    "                                       columns=all_feature_names)\n",
    "X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, \n",
    "                                      columns=all_feature_names)\n",
    "\n",
    "# –°–µ–º–ø–ª –¥–∞–Ω–Ω—ã—Ö –¥–ª—è KernelExplainer\n",
    "X_train_preprocessed_smpl = shap.sample(X_train_preprocessed_df, \n",
    "                                        samples, random_state=RANDOM_STATE)\n",
    "X_test_preprocessed_smpl = shap.sample(X_test_preprocessed_df, \n",
    "                                       samples, random_state=RANDOM_STATE)\n",
    "\n",
    "# –¢–µ–ø–µ—Ä—å, –∫–æ–≥–¥–∞ —É –Ω–∞—Å –µ—Å—Ç—å DataFrame —Å –∏–º–µ–Ω–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å KernelExplainer\n",
    "###explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\n",
    "explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\n",
    "shap_values = explainer.shap_values(X_test_preprocessed_smpl)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Explanation –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ö–õ–ê–°–°–ê.\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values[:,:,1], \n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test_preprocessed_smpl,\n",
    "    feature_names=all_feature_names\n",
    ")\n",
    "\n",
    "shap.summary_plot(shap_values_explanation, plot_size=[15,6])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nshap.summary_plot(shap_values_explanation, plot_size=[15,6], plot_type='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "shap.summary_plot(shap_values_explanation, plot_size=[15,6], plot_type='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nshap.plots.waterfall(shap_values_explanation[0]);\\nshap.plots.waterfall(shap_values_explanation[1]);\\nplt.show()\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "shap.plots.waterfall(shap_values_explanation[0]);\n",
    "shap.plots.waterfall(shap_values_explanation[1]);\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û—Ç—á—ë—Ç –∏ –≤—ã–≤–æ–¥—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- –í —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ–π(—Å–º. –ø–µ—Ä–≤—ã–π —Ä–∞–∑–¥–µ–ª)\n",
    "    - –±—ã–ª–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö —Ä–∞–∑–≤–µ–¥–æ—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∫–æ—Ç–æ—Ä—ã—Ö:\n",
    "        - –±—ã–ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –∏–º–µ–Ω–∞ –ø–æ–ª–µ–π, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö, —É–¥–∞–ª–µ–Ω—ã –º–∞–ª–æ–ø–æ–ª–µ–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤ —Ç. —á. —Å —É—á—ë—Ç–æ–º –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏), –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏, —É–¥–∞–ª–µ–Ω—ã —è–≤–Ω—ã–µ –∏ –Ω–µ—è–≤–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã, ...\n",
    "    - –±—ã–ª–∞ –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—é—â–∞—è ...\n",
    "        - –≤—ã–±—Ä–∞–Ω–∞ –Ω–∞–∏–±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å - **...**, –ø–æ–∫–∞–∑–∞–≤—à–∞—è –Ω–∞–∏–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "            - –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: ...\n",
    "            - —ç—Ç–æ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç –∑–∞–¥–∞–Ω–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - comments legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These icons and [tags] were used to simplify visualization and searching for conclusions, open questions and subtasks, not to forget to finish/check something important. It's convenient to copy a cell from here and paste it into the right place, adding details and then find it by tags or visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑üö©[todo] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü this task is to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑‚úÖ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü the task has been done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑üö®[SOS] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü need help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑üîî[reminder] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü some work to do or check later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è[!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü important constraints or facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° [!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü something interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üè≠ [data transformation] this change ‚§¥ should be done before training/testing/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë∑üößüößüößüößüöß [in progress] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨Ü in progress right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß‚¨áÔ∏èüöß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è some code to be reworked ‚¨ÖÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß‚¨ÜÔ∏èüöß"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 45,
    "start_time": "2025-04-13T00:31:37.601Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-13T00:35:12.169Z"
   },
   {
    "duration": 3185,
    "start_time": "2025-04-16T20:05:37.378Z"
   },
   {
    "duration": 7396,
    "start_time": "2025-04-16T20:06:11.392Z"
   },
   {
    "duration": 1458,
    "start_time": "2025-04-16T20:07:06.981Z"
   },
   {
    "duration": 3384,
    "start_time": "2025-04-16T20:07:10.928Z"
   },
   {
    "duration": 2634,
    "start_time": "2025-04-16T20:07:14.314Z"
   },
   {
    "duration": 3233,
    "start_time": "2025-04-16T20:07:16.950Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-16T20:07:21.523Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:22.279Z"
   },
   {
    "duration": 1126,
    "start_time": "2025-04-16T20:07:24.193Z"
   },
   {
    "duration": 170,
    "start_time": "2025-04-16T20:07:27.323Z"
   },
   {
    "duration": 343,
    "start_time": "2025-04-16T20:07:28.598Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-16T20:07:28.943Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-16T20:07:28.982Z"
   },
   {
    "duration": 135,
    "start_time": "2025-04-16T20:07:29.974Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:30.273Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:30.425Z"
   },
   {
    "duration": 172,
    "start_time": "2025-04-16T20:07:30.924Z"
   },
   {
    "duration": 119,
    "start_time": "2025-04-16T20:07:31.757Z"
   },
   {
    "duration": 1209,
    "start_time": "2025-04-16T20:07:32.102Z"
   },
   {
    "duration": 493,
    "start_time": "2025-04-16T20:07:33.313Z"
   },
   {
    "duration": 173,
    "start_time": "2025-04-16T20:07:34.241Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-16T20:07:34.685Z"
   },
   {
    "duration": 138,
    "start_time": "2025-04-16T20:07:35.869Z"
   },
   {
    "duration": 10897,
    "start_time": "2025-04-16T20:07:36.199Z"
   },
   {
    "duration": 1619,
    "start_time": "2025-04-16T20:07:47.105Z"
   },
   {
    "duration": 362,
    "start_time": "2025-04-16T20:07:48.726Z"
   },
   {
    "duration": 42,
    "start_time": "2025-04-16T20:07:49.090Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-16T20:07:49.134Z"
   },
   {
    "duration": 29,
    "start_time": "2025-04-16T20:07:53.399Z"
   },
   {
    "duration": 27,
    "start_time": "2025-04-16T20:07:55.160Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-16T20:07:57.128Z"
   },
   {
    "duration": 165,
    "start_time": "2025-04-16T20:07:59.059Z"
   },
   {
    "duration": 28,
    "start_time": "2025-04-16T20:08:02.601Z"
   },
   {
    "duration": 1987,
    "start_time": "2025-04-16T20:08:16.912Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-16T20:08:18.905Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-16T20:08:21.456Z"
   },
   {
    "duration": 2051,
    "start_time": "2025-04-16T20:08:30.687Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-16T20:08:35.748Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-16T20:08:36.778Z"
   }
  ],
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
