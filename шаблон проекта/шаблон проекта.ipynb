{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проект сделан на основе выработанного шаблона:\n",
    "- https://github.com/evgeny-ydsc/first-project/blob/main/шаблон%20проекта/шаблон%20проекта.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Название проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "краткое описание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи\n",
    "\n",
    "## Критерии успеха\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## Описание данных\n",
    "\n",
    "\n",
    "\n",
    "- **Целевой признак:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_TASK = 1\n",
    "REGRESSION_TASK = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install phik -q\n",
    "import phik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap -q\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION_TASK:\n",
    "    !pip install lightgbm -q\n",
    "    import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION_TASK:\n",
    "    !pip install category_encoders -q\n",
    "    from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path (csv_name, folder=\"\"):\n",
    "    if not os.path.exists(folder + csv_name):\n",
    "        return '/datasets/' + csv_name\n",
    "    return folder + csv_name\n",
    "\n",
    "def def_load_csv(csv, folder=\"\", print_info=True, **kwargs):\n",
    "    display(f\"⬇-------- Таблица {csv}: ----------⬇\")\n",
    "    df = pd.read_csv(correct_path(csv, folder), **kwargs)\n",
    "    if print_info:\n",
    "        display(df.info())\n",
    "        display(df.head())\n",
    "    return df\n",
    "\n",
    "def columns_to_snake_case (df):\n",
    "    names = {}\n",
    "    for c in df.columns:\n",
    "        names [c] = re.sub(r'(?<!^)(?=[A-Z])', '_', c).lower() \\\n",
    "            .translate(str.maketrans('','',' (){}[]<>/\\\\'))\n",
    "    df.rename(columns=names, inplace=True)\n",
    "    return df\n",
    "\n",
    "# визуализирует все категориальные фичи датафрейма в виде гистограммы TOP-значений\n",
    "# визуализирует все количественные фичи датафрейма в виде ящиков с усами и гистограмм с распределением, выводит descirbe\n",
    "# поддерживает любое количество фильтров, фильтры поочерёдно применяются к датафрейму как запрос в query()\n",
    "# если sort_filter_index=-1, то порядок столбцов сортируются их длине (суммарно по фильтрам)\n",
    "# ignore_columns позволяет избежать рассчёта статистик для столбцов-идентификаторов\n",
    "def do_eda(df, columns=[], ignore_columns=[\"id\"], filters=[], sort_filter_index=-1, top_categories=10):\n",
    "    bins=50\n",
    "    n_filters = len(filters)\n",
    "    \n",
    "    if len(columns) == 0:\n",
    "        columns = df.columns\n",
    "\n",
    "    # вывод графиков по категориальным фичам \n",
    "    for column in columns:\n",
    "        if ( df[column].dtype == 'O' or df[column].dtype.name == 'category') \\\n",
    "                and column not in ignore_columns:\n",
    "            display (f\"⬇-------- столбец {column} --------⬇\")\n",
    "            title0 = f\"столбец {column}, количество и доля значений\"\n",
    "            if n_filters == 0:\n",
    "                df_show = pd.DataFrame(df.groupby(column)[column].count(),\n",
    "                    columns = [column])\n",
    "                df_show.index.name = ''\n",
    "                df_show = df_show.sort_values( column, ascending=False)\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i, _ in enumerate(filters):\n",
    "                    df_f = df.query(filters[i]).groupby(column)[column].count()\n",
    "                    df_f = pd.DataFrame({f'фильтр: {filters[i]}':df_f})\n",
    "                    df_f.index.name = ''\n",
    "                    df_show = df_show.join(df_f, how='outer')\n",
    "                df_show = df_show.fillna(0)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show['all_cat_agg_tmp'] = df_show.sum(axis=1)\n",
    "                df_show = df_show.sort_values( \\\n",
    "                    df_show.columns[sort_filter_index], ascending=False)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show = df_show.drop (columns=['all_cat_agg_tmp'], axis=1)\n",
    "            # sum not-top categories in a single row\n",
    "            if top_categories + 1 < df_show.shape[0]:\n",
    "                df_show_other = df_show.iloc [top_categories:]\n",
    "                df_show = df_show.iloc [:top_categories]\n",
    "                df_show.loc[f'All other {df_show_other.shape[0]} categories'] = df_show_other.sum()\n",
    "\n",
    "            ax = df_show.plot.barh(title=title0, stacked=(n_filters != 0));\n",
    "            ax.invert_yaxis()\n",
    "            df_show ['sum_cols'] = df_show.sum(axis=1)\n",
    "            sum_all = df_show.iloc[:,-1].sum()\n",
    "            for i in range(df_show.shape[0]):\n",
    "                sum_row = df_show.iloc[i, -1]\n",
    "                s = f'{sum_row} ({int(sum_row*100.0/sum_all)}'\n",
    "                for j in range(n_filters):\n",
    "                    s += '=' if j == 0 else '+'\n",
    "                    s += f'{int(df_show.iloc[i,j]*100.0/sum_all)}'\n",
    "                ax.text(int(sum_row*1.05), i, s+')%')\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ax.set_xlim(xmin, 1.5*xmax)\n",
    "            ax.legend(bbox_to_anchor=(1.8, 0.7))\n",
    "            plt.show();\n",
    "\n",
    "    # вывод графиков по числовым фичам \n",
    "    for column in columns:\n",
    "        if df[column].dtype in ['int64', 'float64'] and column not in ignore_columns:\n",
    "            display (f\"⬇-------- Следующий признак: {column} --------⬇\")\n",
    "            title = f\"столбец {column}\"\n",
    "            _, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 7))\n",
    "            if n_filters == 0:\n",
    "                df_show = df[[column]]\n",
    "                display(df_show.describe())\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i in range (n_filters):\n",
    "                    df_f = df.query(filters[i]).copy()\n",
    "                    f_title = f\"фильтр: {filters[i]}\"\n",
    "                    df_f.loc[:, 'filter'] = f_title\n",
    "                    df_f = df_f[['filter', column]]\n",
    "                    df_show = pd.concat([df_show, df_f], axis=0, ignore_index=True)\n",
    "                    display(f_title)\n",
    "                    display(df_f[column].describe())\n",
    "\n",
    "            ax = sns.boxplot(data=df_show, orient=\"h\", ax=axs[0], y = None if n_filters == 0 else 'filter', x = column)\n",
    "            sns.histplot(data=df_show, x=column, hue= None if n_filters == 0 else 'filter', multiple=\"dodge\",\n",
    "                        bins=bins, kde=True, ax=axs[1], alpha=0.8, legend=False)\n",
    "            plt.show();\n",
    "\n",
    "def display_drop_full_duplicates(df):\n",
    "    display(f'{df.duplicated().sum()} full duplicates are dropped')\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def print_unique_text_features(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            x = df[c].unique()\n",
    "            x = np.sort(x[~pd.isnull(x)])\n",
    "            display(f\"⬇-------- Feature \\\"{c}\\\"--------⬇\")\n",
    "            display(x)\n",
    "\n",
    "# example:\n",
    "# df = split_date (df, 'created_at', needed_parts='ymdwh', format='%Y-%m-%d %H:%M:%S'),\n",
    "# use it after solving NaNs to prevent float types of the new fields\n",
    "def split_date (df, date_column, needed_parts, format=\"\", drop_date_column=False):\n",
    "    if format != \"\":\n",
    "        df[date_column] = pd.to_datetime(df[date_column], format=format)\n",
    "    if 'y' in needed_parts:\n",
    "        df [date_column + \"_year\"] = df[date_column].dt.year\n",
    "    if 'm' in needed_parts:\n",
    "        df [date_column + \"_month\"] = df[date_column].dt.month\n",
    "    if 'd' in needed_parts:\n",
    "        df [date_column + \"_day\"] = df[date_column].dt.day\n",
    "    if 'w' in needed_parts:\n",
    "        df [date_column + \"_weekday\"] = df[date_column].dt.weekday\n",
    "    if 'h' in needed_parts:\n",
    "        df [date_column + \"_hour\"] = df[date_column].dt.hour\n",
    "    if drop_date_column:\n",
    "        df = df.drop([date_column], axis=1)\n",
    "    return df\n",
    "\n",
    "def show_nans (df):\n",
    "    n = df.isna().sum()\n",
    "    l = df.shape[0]\n",
    "    display(\"empty parts:\")\n",
    "    display(n [n>0].map(lambda x: f'{round(x/l*100, 2)}%'))\n",
    "\n",
    "def fill_by_median (df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "def fill_by_value (df, columns, val):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(val)\n",
    "    return df\n",
    "\n",
    "def get_multicollinear_features (phik_matrix):\n",
    "    mc_features=[]\n",
    "    while True:\n",
    "        if phik_matrix.shape[0] == 0:\n",
    "            break\n",
    "        mc_features_ranked = phik_matrix[(phik_matrix >= 0.9) & (phik_matrix < 1)].sum()\n",
    "        mc_features_ranked = mc_features_ranked[mc_features_ranked>0] \\\n",
    "            .sort_values(ascending=False)\n",
    "        if mc_features_ranked.shape[0] == 0:\n",
    "            break\n",
    "        top_mc_feature = mc_features_ranked.index[0]\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=0)\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=1)\n",
    "        mc_features.append(top_mc_feature)\n",
    "    return mc_features, phik_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка главных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_path (csv_name, folder=\"\"):\n",
    "    if not os.path.exists(folder + csv_name):\n",
    "        return '/datasets/' + csv_name\n",
    "    return folder + csv_name\n",
    "\n",
    "def def_load_csv(csv, folder=\"\", print_info=True):\n",
    "    display(f\"⬇-------- Таблица {csv}: ----------⬇\")\n",
    "    df = pd.read_csv(correct_path(csv, folder))\n",
    "    if print_info:\n",
    "        display(df.info())\n",
    "        display(df.head())\n",
    "    return df\n",
    "\n",
    "def columns_to_snake_case (df):\n",
    "    names = {}\n",
    "    for c in df.columns:\n",
    "        names [c] = re.sub(r'(?<!^)(?=[A-Z])', '_', c).lower() \\\n",
    "            .translate(str.maketrans('','',' (){}[]<>/\\\\'))\n",
    "    df.rename(columns=names, inplace=True)\n",
    "    return df\n",
    "\n",
    "# визуализирует все категориальные фичи датафрейма в виде гистограммы TOP-значений\n",
    "# визуализирует все количественные фичи датафрейма в виде ящиков с усами и гистограмм с распределением, выводит descirbe\n",
    "# поддерживает любое количество фильтров, фильтры поочерёдно применяются к датафрейму как запрос в query()\n",
    "# если sort_filter_index=-1, то порядок столбцов сортируются их длине (суммарно по фильтрам)\n",
    "# ignore_columns позволяет избежать рассчёта статистик для столбцов-идентификаторов\n",
    "def do_eda(df, columns=[], ignore_columns=[\"id\"], filters=[], sort_filter_index=-1, top_categories=10):\n",
    "    bins=25\n",
    "    n_filters = len(filters)\n",
    "    \n",
    "    if len(columns) == 0:\n",
    "        columns = df.columns\n",
    "\n",
    "    # вывод графиков по категориальным фичам \n",
    "    for column in columns:\n",
    "        if ( df[column].dtype == 'O' or df[column].dtype.name == 'category') \\\n",
    "                and column not in ignore_columns:\n",
    "            display (f\"⬇-------- столбец {column} --------⬇\")\n",
    "            title0 = f\"столбец {column}, количество и доля значений\"\n",
    "            if n_filters == 0:\n",
    "                df_show = pd.DataFrame(df.groupby(column)[column].count(),\n",
    "                    columns = [column])\n",
    "                df_show.index.name = ''\n",
    "                df_show = df_show.sort_values( column, ascending=False)\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i, _ in enumerate(filters):\n",
    "                    df_f = df.query(filters[i]).groupby(column)[column].count()\n",
    "                    df_f = pd.DataFrame({f'фильтр: {filters[i]}':df_f})\n",
    "                    df_f.index.name = ''\n",
    "                    df_show = df_show.join(df_f, how='outer')\n",
    "                df_show = df_show.fillna(0)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show['all_cat_agg_tmp'] = df_show.sum(axis=1)\n",
    "                df_show = df_show.sort_values( \\\n",
    "                    df_show.columns[sort_filter_index], ascending=False)\n",
    "                if sort_filter_index == -1:\n",
    "                    df_show = df_show.drop (columns=['all_cat_agg_tmp'], axis=1)\n",
    "            # sum not-top categories in a single row\n",
    "            if top_categories + 1 < df_show.shape[0]:\n",
    "                df_show_other = df_show.iloc [top_categories:]\n",
    "                df_show = df_show.iloc [:top_categories]\n",
    "                df_show.loc[f'All other {df_show_other.shape[0]} categories'] = df_show_other.sum()\n",
    "\n",
    "            ax = df_show.plot.barh(title=title0, stacked=(n_filters != 0));\n",
    "            ax.invert_yaxis()\n",
    "            df_show ['sum_cols'] = df_show.sum(axis=1)\n",
    "            sum_all = df_show.iloc[:,-1].sum()\n",
    "            for i in range(df_show.shape[0]):\n",
    "                sum_row = df_show.iloc[i, -1]\n",
    "                s = f'{sum_row} ({int(sum_row*100.0/sum_all)}'\n",
    "                for j in range(n_filters):\n",
    "                    s += '=' if j == 0 else '+'\n",
    "                    s += f'{int(df_show.iloc[i,j]*100.0/sum_all)}'\n",
    "                ax.text(int(sum_row*1.05), i, s+')%')\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ax.set_xlim(xmin, 1.5*xmax)\n",
    "            ax.legend(bbox_to_anchor=(1.8, 0.7))\n",
    "            plt.show();\n",
    "\n",
    "    # вывод графиков по числовым фичам \n",
    "    for column in columns:\n",
    "        if df[column].dtype in ['int64', 'float64'] and column not in ignore_columns:\n",
    "            display (f\"⬇-------- Следующий признак: {column} --------⬇\")\n",
    "            title = f\"столбец {column}\"\n",
    "            _, axs = plt.subplots(nrows=2, ncols=1, figsize=(6, 7))\n",
    "            if n_filters == 0:\n",
    "                df_show = df[[column]]\n",
    "                display(df_show.describe())\n",
    "            else:\n",
    "                df_show = pd.DataFrame()\n",
    "                for i in range (n_filters):\n",
    "                    df_f = df.query(filters[i]).copy()\n",
    "                    f_title = f\"фильтр: {filters[i]}\"\n",
    "                    df_f.loc[:, 'filter'] = f_title\n",
    "                    df_f = df_f[['filter', column]]\n",
    "                    df_show = pd.concat([df_show, df_f], axis=0, ignore_index=True)\n",
    "                    display(f_title)\n",
    "                    display(df_f[column].describe())\n",
    "\n",
    "            ax = sns.boxplot(data=df_show, orient=\"h\", ax=axs[0], y = None if n_filters == 0 else 'filter', x = column)\n",
    "            sns.histplot(data=df_show, x=column, hue= None if n_filters == 0 else 'filter', multiple=\"dodge\",\n",
    "                        bins=bins, kde=True, ax=axs[1], alpha=0.8, legend=False)\n",
    "            plt.show();\n",
    "\n",
    "def display_drop_full_duplicates(df):\n",
    "    display(f'{df.duplicated().sum()} full duplicates are dropped')\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def print_unique_text_features(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == 'O':\n",
    "            x = df[c].unique()\n",
    "            x = np.sort(x[~pd.isnull(x)])\n",
    "            display(f\"⬇-------- Feature \\\"{c}\\\"--------⬇\")\n",
    "            display(x)\n",
    "\n",
    "# example:\n",
    "# df = split_date (df, 'created_at', needed_parts='ymdwh', format='%Y-%m-%d %H:%M:%S'),\n",
    "# use it after solving NaNs to prevent float types of the new fields\n",
    "def split_date (df, date_column, needed_parts, format=\"\", drop_date_column=False):\n",
    "    if format != \"\":\n",
    "        df[date_column] = pd.to_datetime(df[date_column], format=format)\n",
    "    if 'y' in needed_parts:\n",
    "        df [date_column + \"_year\"] = df[date_column].dt.year\n",
    "    if 'm' in needed_parts:\n",
    "        df [date_column + \"_month\"] = df[date_column].dt.month\n",
    "    if 'd' in needed_parts:\n",
    "        df [date_column + \"_day\"] = df[date_column].dt.day\n",
    "    if 'w' in needed_parts:\n",
    "        df [date_column + \"_weekday\"] = df[date_column].dt.weekday\n",
    "    if 'h' in needed_parts:\n",
    "        df [date_column + \"_hour\"] = df[date_column].dt.hour\n",
    "    if drop_date_column:\n",
    "        df = df.drop([date_column], axis=1)\n",
    "    return df\n",
    "\n",
    "def show_nans (df):\n",
    "    n = df.isna().sum()\n",
    "    l = df.shape[0]\n",
    "    display(\"empty parts:\")\n",
    "    display(n [n>0].map(lambda x: f'{round(x/l*100, 2)}%'))\n",
    "\n",
    "def fill_by_median (df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "def fill_by_value (df, columns, val):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(val)\n",
    "    return df\n",
    "\n",
    "def get_multicollinear_features (phik_matrix):\n",
    "    mc_features=[]\n",
    "    while True:\n",
    "        if phik_matrix.shape[0] == 0:\n",
    "            break\n",
    "        mc_features_ranked = phik_matrix[(phik_matrix >= 0.9) & (phik_matrix < 1)].sum()\n",
    "        mc_features_ranked = mc_features_ranked[mc_features_ranked>0] \\\n",
    "            .sort_values(ascending=False)\n",
    "        if mc_features_ranked.shape[0] == 0:\n",
    "            break\n",
    "        top_mc_feature = mc_features_ranked.index[0]\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=0)\n",
    "        phik_matrix = phik_matrix.drop(top_mc_feature,axis=1)\n",
    "        mc_features.append(top_mc_feature)\n",
    "    return mc_features, phik_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка таблиц, исправление сепараторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = def_load_csv ('....csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поправим имена полей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = columns_to_snake_case(df)\\ndisplay (df.info())\\ndisplay (df.head())\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = columns_to_snake_case(df)\n",
    "display (df.info())\n",
    "display (df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исправим типы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['...'] = pd.to_datetime( df['...'], format='%Y-%m-%d %H:%M:%S' )\n",
    "#df['...'] = df['...'].astype('object')\n",
    "#df['..._year'] = df['...'].dt.year\n",
    "#df['..._month'] = df['...'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndisplay(df.info())\\ndisplay(df.head())\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "display(df.info())\n",
    "display(df.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удалим поля, приводящие к утечке данных или заведомо бесполезные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `...` - ... комментарий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['...', '...'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изучим пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = df.isna().sum()\\nl = len(df)\\nround(n [n>0] / l, 2)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "show_nans (df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf['vehicle_type'] = df['vehicle_type']     .fillna(df.groupby(['brand', 'model'])['vehicle_type']             .transform(lambda x: x.mode()[0] if not x.mode().empty else None))\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df['vehicle_type'] = df['vehicle_type'] \\\n",
    "    .fillna(df.groupby(['brand', 'model'])['vehicle_type'] \\\n",
    "            .transform(lambda x: x.mode()[0] if not x.mode().empty else None))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷🚩[todo] если же какие-то пропуски всё-таки останутся - избавимся от них с помощью imputer-ов в pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изучим и удалим явные дубликаты в датафрейме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndisplay(df.duplicated().sum())\\ndf = df.drop_duplicates()\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "display(df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изучим неявные дубликаты в датафрейме\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_unique_text_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим значения xxx фичи f на yyy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df['f']=='xxx', 'f'] = 'yyy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравним соответствующие фичи в тренировочной и тестовой выборках если они из разных источников, а не из одной таблицы(состав, типы, значения) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследовательский анализ данных (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#do_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы и задачи по итогам EDA\n",
    "1. ...\n",
    "    1. 👷🚩[todo] ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорректируем данные, заменив аномалии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf.loc[(df['xxx'] <10) | (df['xxx'] > 1600), 'xxx'] =     df.groupby(['brand', 'model'])['power']         .transform(lambda x: x.mode()[0] if not x.mode().empty else None)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df.loc[(df['xxx'] <10) | (df['xxx'] > 1600), 'xxx'] = \\\n",
    "    df.groupby(['brand', 'model'])['power'] \\\n",
    "        .transform(lambda x: x.mode()[0] if not x.mode().empty else None)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорректируем данные, удалив аномалии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.query('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужные столбцы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolumns_to_delete = ['...', '...']\\nX_train = X_train.drop(columns_to_delete, axis=1)\\nX_test = X_test.drop(columns_to_delete, axis=1)\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "columns_to_delete = ['...', '...']\n",
    "df = df.drop(columns_to_delete, axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверим мультиколлинеарность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n_, ax = plt.subplots(figsize=(25,25))\\nph_mx = X_train.phik_matrix(interval_cols=['kilometer', 'power', 'registration_year'],\\n                bins={'kilometer':10, 'power':10, 'registration_year':10})\\nsns.heatmap(ph_mx, annot=True, fmt='.2f', ax=ax);\\n\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "interval_cols=['xxx', 'xxx', 'xxx', 'xxx']\n",
    "   \n",
    "bins = {}\n",
    "for c in interval_cols:\n",
    "    bins[c] = 10;\n",
    "\n",
    "_, ax = plt.subplots(figsize=(25,25))\n",
    "ph_mx = X_train.phik_matrix(interval_cols=interval_cols, bins=bins)\n",
    "sns.heatmap(ph_mx, annot=True, fmt='.2f', ax=ax);\n",
    "\n",
    "columns_to_delete, ph_mx_no_mc = get_multicollinear_features (ph_mx)\n",
    "display (\"Мультиколлинеарные фичи для удаления:\")\n",
    "display (columns_to_delete)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Как правило, о мультиколлинеарности говорят при значении коэффициента корреляции от 0.9 до 0.95 по модулю. В такой ситуации лишние коррелирующие признаки нужно удалять из обучающей выборки.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужные столбцы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncolumns_to_delete = ['...', '...']\\nX_train = X_train.drop(columns_to_delete, axis=1)\\nX_test = X_test.drop(columns_to_delete, axis=1)\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "columns_to_delete = ['...', '...']\n",
    "X_train = X_train.drop(columns_to_delete, axis=1)\n",
    "X_test = X_test.drop(columns_to_delete, axis=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разобъём выборку на тренировочную и тестовую (со стратификацией по целевому признаку в случае классификации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX = df.drop(['price'], axis=1)\\ny = df['price']\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, \\n    y, \\n    random_state=RANDOM_STATE\\n)\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверим сбалансированность тренировочной выборки относительно целевого класса (в случае классификации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_columns = ['...', '...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor c in X_train.columns:\\n    if X_train[c].dtype == \\'O\\':\\n        X_train[c] = X_train[c].astype(\"category\")\\ndisplay(X_train.info())\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for c in X_train.columns:\n",
    "    if X_train[c].dtype == 'O':\n",
    "        X_train[c] = X_train[c].astype(\"category\")\n",
    "display(X_train.info())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cat_columns = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "ord_columns = ['weather_1', 'road_surface', 'road_condition_1', 'lighting ', 'control_device']\n",
    "cat_columns = list(set(cat_columns) - set(ord_columns))\n",
    "num_features = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncat_pipe = Pipeline(\\n    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\\n     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\\n    ]\\n    )\\n\\ndata_preprocessor = ColumnTransformer(\\n    [\\n        ('ohe', cat_pipe, cat_columns),\\n        ('num', StandardScaler(), num_columns),\\n    ], \\n    remainder='passthrough'\\n) \\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ohe_pipe = Pipeline(\n",
    "    [('simpleImputer_ohe', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "     ('ohe', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "ord_pipe = Pipeline(\n",
    "    [('simpleImputer_before_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
    "     ('ord',  OrdinalEncoder(\n",
    "                categories=[\n",
    "                    ['clear', 'cloudy', 'other', '-', 'wind', 'raining', 'snowing', 'fog'], \n",
    "                    ['dry', 'wet', '-', 'slippery', 'snowy'], \n",
    "                    ['normal', 'other', '-', 'reduced width', 'flooded', 'holes', 'loose material', 'obstruction', 'construction'], \n",
    "                    ['daylight', 'dark with street lights', '-', 'dark with street lights not functioning', 'dark with no street lights', 'dusk or dawn'], \n",
    "                    ['functioning', 'obscured', 'not functioning', 'none']\n",
    "                ], \n",
    "                handle_unknown='use_encoded_value', unknown_value=np.nan\n",
    "            )\n",
    "        ),\n",
    "     ('simpleImputer_after_ord', SimpleImputer(missing_values=np.nan, strategy='most_frequent'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('ohe', ohe_pipe, cat_columns),\n",
    "        ('ord', ord_pipe, ord_columns),\n",
    "        ('num', StandardScaler(), num_columns),\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ") \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pipe_final = Pipeline([\n",
    "    ('preprocessor', data_preprocessor),\n",
    "    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
    "]\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%%time\\n\\npipe_final = Pipeline([\\n    ('preprocessor', data_preprocessor),\\n    ('models', DecisionTreeClassifier(random_state=RANDOM_STATE))\\n]\\n)\\n\\nparam_grid = [\\n    {\\n        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')],\\n        'models__max_depth': [4,7],\\n        'models__max_features': [10, 50],\\n        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],\\n        'models__max_depth': [10, 20],\\n        'models__min_samples_split': [5, 10],\\n        'models__min_samples_leaf': [2, 4],\\n        'models__bootstrap': [True, False],\\n        'preprocessor__num': [MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [LogisticRegression(\\n            random_state=RANDOM_STATE, \\n            solver='liblinear', \\n            penalty='l1',\\n            class_weight='balanced'\\n        )],\\n        'models__C': range(1, 3),\\n        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \\n    }\\n    ,\\n    {\\n        'models': [SVC(\\n            random_state=RANDOM_STATE, \\n            kernel='linear', probability=True, class_weight='balanced'\\n        )],\\n        'models__kernel': ['poly', 'rbf'],\\n        'models__degree': [2, 5, 10],\\n        'models__C': range(2, 3),\\n    }\\n]\\n\\nsearch = RandomizedSearchCV(\\n    pipe_final, \\n    param_grid,\\n    cv=3,\\n    scoring='f1',\\n    n_iter=30,\\n    verbose = 10,\\n    n_jobs=-1\\n)\\n\\nsearch.fit(X_train, y_train)\\n\\ngrid_search.best_params_\\n\\nbest_model = search.best_estimator_\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%%time\n",
    "param_grid = [\n",
    "    {\n",
    "        'models': [DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')],\n",
    "        'models__max_depth': [4,7],\n",
    "        'models__max_features': [10, 50],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [RandomForestClassifier(random_state=RANDOM_STATE)],\n",
    "        'models__max_depth': [10, 20],\n",
    "        'models__min_samples_split': [5, 10],\n",
    "        'models__min_samples_leaf': [2, 4],\n",
    "        'models__bootstrap': [True, False],\n",
    "        'preprocessor__num': [MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [LogisticRegression(\n",
    "            random_state=RANDOM_STATE, \n",
    "            solver='liblinear', \n",
    "            penalty='l1',\n",
    "            class_weight='balanced'\n",
    "        )],\n",
    "        'models__C': range(1, 3),\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [SVC(\n",
    "            random_state=RANDOM_STATE, \n",
    "            kernel='linear', probability=True, class_weight='balanced'\n",
    "        )],\n",
    "        'models__kernel': ['poly', 'rbf'],\n",
    "        'models__degree': [2, 5, 10],\n",
    "        'models__C': range(2, 3),\n",
    "    }\n",
    "]\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipe_final, \n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_iter=30,\n",
    "    verbose = 10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "param_grid = [\n",
    "    {\n",
    "        'models': [LinearRegression()],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    }\n",
    "    ,\n",
    "    {\n",
    "        'models': [DecisionTreeRegressor(random_state=RANDOM_STATE)],\n",
    "        'models__max_depth': [3, 5, 10, None],\n",
    "        'models__min_samples_split': [2, 5, 10],\n",
    "        'preprocessor__num': [StandardScaler(), MinMaxScaler()]   \n",
    "    },\n",
    "    {\n",
    "        'models': [SVR()],\n",
    "        'models__kernel': ['linear', 'rbf'],\n",
    "        'models__C': [0.1, 1, 10],\n",
    "        'models__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    {\n",
    "        'models': [RandomForestRegressor(random_state=RANDOM_STATE)],\n",
    "        'models__n_estimators': [50, 100],\n",
    "        'models__max_depth': [None, 10, 20],\n",
    "        'models__min_samples_split': [2, 5]\n",
    "    }\n",
    "]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display ('Метрика качества лучшей модели на кроссвалидации: ', search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики качества на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_pred = best_model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# оценка классификации\n",
    "'''\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ny_pred = best_model.predict(X_test)\\ndisplay ('RMSE на тестовой выборке = ', root_mean_squared_error(y_test, y_pred))\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# оценка регрессии\n",
    "'''\n",
    "y_pred = best_model.predict(X_test)\n",
    "display ('RMSE на тестовой выборке = ', root_mean_squared_error(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "'''\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test);\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrecisionRecallDisplay\n",
    "'''\n",
    "RocCurveDisplay.from_estimator(best_model, X_test, y_test);\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ остатков (в случае регрессии)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresiduals = y_test - y_pred\\n\\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\\naxes[0].hist(residuals, bins=33)\\naxes[0].set_title('Гистограмма распределения остатков')\\naxes[0].set_xlabel('Остатки')\\n\\naxes[1].scatter(predictions, residuals)\\naxes[1].set_xlabel('Предсказания модели')\\naxes[1].set_ylabel('Остатки')\\naxes[1].set_title('Анализ дисперсии')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "axes[0].hist(residuals, bins=33)\n",
    "axes[0].set_title('Гистограмма распределения остатков')\n",
    "axes[0].set_xlabel('Остатки')\n",
    "\n",
    "axes[1].scatter(y_pred, residuals)\n",
    "axes[1].set_xlabel('Предсказания модели')\n",
    "axes[1].set_ylabel('Остатки')\n",
    "axes[1].set_title('Анализ дисперсии')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остатки модели должны быть нормально распределены, симметрично располагаться на графике, а также иметь постоянную дисперсию для всех значений предсказаний.\n",
    "\n",
    "Для всех случаев ненормального распределения данных подходят два совета:\n",
    "- Найти и добавить в модель дополнительные признаки. Скорее всего, на этапе сбора данных не была учтена какая-то информация.\n",
    "- Трансформировать данные, чтобы изменить взаимосвязь входных признаков с целевым. Этим преобразованиям вы научитесь в следующем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ важности фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsamples = 20\\n \\n# Извлечение лучшей модели из результатов RandomizedSearchCV\\nbest_model = search.best_estimator_.named_steps['models']\\n\\n# Предобработка данных через пайплайн без конечной модели\\npreprocessor = search.best_estimator_.named_steps['preprocessor']\\nX_train_preprocessed = preprocessor.transform(X_train)\\nX_test_preprocessed = preprocessor.transform(X_test)\\n\\nall_feature_names = preprocessor.get_feature_names_out()\\n\\n# Создаем DataFrame с соответствующими именами колонок\\nX_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, \\n                                       columns=all_feature_names)\\nX_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, \\n                                      columns=all_feature_names)\\n\\n# Семпл данных для KernelExplainer\\nX_train_preprocessed_smpl = shap.sample(X_train_preprocessed_df, \\n                                        samples, random_state=RANDOM_STATE)\\nX_test_preprocessed_smpl = shap.sample(X_test_preprocessed_df, \\n                                       samples, random_state=RANDOM_STATE)\\n\\n# Теперь, когда у нас есть DataFrame с именами признаков, мы можем использовать KernelExplainer\\n###explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\\nexplainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\\nshap_values = explainer.shap_values(X_test_preprocessed_smpl)\\n\\n# Создаем объект Explanation для первого КЛАССА.\\nshap_values_explanation = shap.Explanation(\\n    values=shap_values[:,:,1], \\n    base_values=explainer.expected_value,\\n    data=X_test_preprocessed_smpl,\\n    feature_names=all_feature_names\\n)\\n\\nshap.summary_plot(shap_values_explanation, plot_size=[15,6])\\nplt.show()\\n\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "samples = 20\n",
    " \n",
    "# Извлечение лучшей модели из результатов RandomizedSearchCV\n",
    "best_model = search.best_estimator_.named_steps['models']\n",
    "\n",
    "# Предобработка данных через пайплайн без конечной модели\n",
    "preprocessor = search.best_estimator_.named_steps['preprocessor']\n",
    "X_train_preprocessed = preprocessor.transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "all_feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Создаем DataFrame с соответствующими именами колонок\n",
    "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, \n",
    "                                       columns=all_feature_names)\n",
    "X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, \n",
    "                                      columns=all_feature_names)\n",
    "\n",
    "# Семпл данных для KernelExplainer\n",
    "X_train_preprocessed_smpl = shap.sample(X_train_preprocessed_df, \n",
    "                                        samples, random_state=RANDOM_STATE)\n",
    "X_test_preprocessed_smpl = shap.sample(X_test_preprocessed_df, \n",
    "                                       samples, random_state=RANDOM_STATE)\n",
    "\n",
    "# Теперь, когда у нас есть DataFrame с именами признаков, мы можем использовать KernelExplainer\n",
    "###explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\n",
    "explainer = shap.KernelExplainer(best_model.predict_proba, X_train_preprocessed_smpl)\n",
    "shap_values = explainer.shap_values(X_test_preprocessed_smpl)\n",
    "\n",
    "# Создаем объект Explanation для первого КЛАССА.\n",
    "shap_values_explanation = shap.Explanation(\n",
    "    values=shap_values[:,:,1], \n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test_preprocessed_smpl,\n",
    "    feature_names=all_feature_names\n",
    ")\n",
    "\n",
    "shap.summary_plot(shap_values_explanation, plot_size=[15,6])\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nshap.summary_plot(shap_values_explanation, plot_size=[15,6], plot_type='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "shap.summary_plot(shap_values_explanation, plot_size=[15,6], plot_type='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nshap.plots.waterfall(shap_values_explanation[0]);\\nshap.plots.waterfall(shap_values_explanation[1]);\\nplt.show()\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "shap.plots.waterfall(shap_values_explanation[0]);\n",
    "shap.plots.waterfall(shap_values_explanation[1]);\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчёт и выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- В соответствии с поставленной задачей(см. первый раздел)\n",
    "    - была проведена подготовка данных и их разведочный анализ, в результате которых:\n",
    "        - были исправлены имена полей, исправлены типы данных, удалены малополезные признаки (в т. ч. с учётом мультиколлинеарности), заполнены пропуски, удалены явные и неявные дубликаты, ...\n",
    "    - была натренирована модель, предсказывающая ...\n",
    "        - выбрана наиболее оптимальная модель - **...**, показавшая наилучшие результаты:\n",
    "            - качество предсказания: ...\n",
    "            - это удовлетворяет заданный критерий качества: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - comments legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These icons and [tags] were used to simplify visualization and searching for conclusions, open questions and subtasks, not to forget to finish/check something important. It's convenient to copy a cell from here and paste it into the right place, adding details and then find it by tags or visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷🚩[todo] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ this task is to be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷✅ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ the task has been done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷🚨[SOS] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ need help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷🔔[reminder] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ some work to do or check later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️[!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ important constraints or facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 [!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ something interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🏭 [data transformation] this change ⤴ should be done before training/testing/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👷🚧🚧🚧🚧🚧 [in progress] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⬆ in progress right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧⬇️🚧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ some code to be reworked ⬅️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧⬆️🚧"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 45,
    "start_time": "2025-04-13T00:31:37.601Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-13T00:35:12.169Z"
   },
   {
    "duration": 3185,
    "start_time": "2025-04-16T20:05:37.378Z"
   },
   {
    "duration": 7396,
    "start_time": "2025-04-16T20:06:11.392Z"
   },
   {
    "duration": 1458,
    "start_time": "2025-04-16T20:07:06.981Z"
   },
   {
    "duration": 3384,
    "start_time": "2025-04-16T20:07:10.928Z"
   },
   {
    "duration": 2634,
    "start_time": "2025-04-16T20:07:14.314Z"
   },
   {
    "duration": 3233,
    "start_time": "2025-04-16T20:07:16.950Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-16T20:07:21.523Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:22.279Z"
   },
   {
    "duration": 1126,
    "start_time": "2025-04-16T20:07:24.193Z"
   },
   {
    "duration": 170,
    "start_time": "2025-04-16T20:07:27.323Z"
   },
   {
    "duration": 343,
    "start_time": "2025-04-16T20:07:28.598Z"
   },
   {
    "duration": 2,
    "start_time": "2025-04-16T20:07:28.943Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-16T20:07:28.982Z"
   },
   {
    "duration": 135,
    "start_time": "2025-04-16T20:07:29.974Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:30.273Z"
   },
   {
    "duration": 15,
    "start_time": "2025-04-16T20:07:30.425Z"
   },
   {
    "duration": 172,
    "start_time": "2025-04-16T20:07:30.924Z"
   },
   {
    "duration": 119,
    "start_time": "2025-04-16T20:07:31.757Z"
   },
   {
    "duration": 1209,
    "start_time": "2025-04-16T20:07:32.102Z"
   },
   {
    "duration": 493,
    "start_time": "2025-04-16T20:07:33.313Z"
   },
   {
    "duration": 173,
    "start_time": "2025-04-16T20:07:34.241Z"
   },
   {
    "duration": 30,
    "start_time": "2025-04-16T20:07:34.685Z"
   },
   {
    "duration": 138,
    "start_time": "2025-04-16T20:07:35.869Z"
   },
   {
    "duration": 10897,
    "start_time": "2025-04-16T20:07:36.199Z"
   },
   {
    "duration": 1619,
    "start_time": "2025-04-16T20:07:47.105Z"
   },
   {
    "duration": 362,
    "start_time": "2025-04-16T20:07:48.726Z"
   },
   {
    "duration": 42,
    "start_time": "2025-04-16T20:07:49.090Z"
   },
   {
    "duration": 32,
    "start_time": "2025-04-16T20:07:49.134Z"
   },
   {
    "duration": 29,
    "start_time": "2025-04-16T20:07:53.399Z"
   },
   {
    "duration": 27,
    "start_time": "2025-04-16T20:07:55.160Z"
   },
   {
    "duration": 6,
    "start_time": "2025-04-16T20:07:57.128Z"
   },
   {
    "duration": 165,
    "start_time": "2025-04-16T20:07:59.059Z"
   },
   {
    "duration": 28,
    "start_time": "2025-04-16T20:08:02.601Z"
   },
   {
    "duration": 1987,
    "start_time": "2025-04-16T20:08:16.912Z"
   },
   {
    "duration": 4,
    "start_time": "2025-04-16T20:08:18.905Z"
   },
   {
    "duration": 16,
    "start_time": "2025-04-16T20:08:21.456Z"
   },
   {
    "duration": 2051,
    "start_time": "2025-04-16T20:08:30.687Z"
   },
   {
    "duration": 7,
    "start_time": "2025-04-16T20:08:35.748Z"
   },
   {
    "duration": 11,
    "start_time": "2025-04-16T20:08:36.778Z"
   }
  ],
  "kernelspec": {
   "display_name": "practicum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
